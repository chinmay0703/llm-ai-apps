# -*- coding: utf-8 -*-
"""ğŸ“Watsapp_Chat_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16C8bmNzMcE49xVAdpWWnv5erVqQ9by7a
"""

!apt update && apt install -y curl git

!curl -fsSL https://ollama.com/install.sh | sh

!ollama pull llama3.2

!pkill -f ngrok  # Kills all ngrok processes
!fuser -k 8501/tcp  # Kills any process using port 6000 (or use 5000 if needed)
# Install required libraries
OLLAMA_API_URL = "http://127.0.0.1:11434"
!nohup ollama serve > /dev/null 2>&1 &

!pip install streamlit PyPDF2 requests python-docx

!pip install pyngrok

from pyngrok import ngrok


# Replace with your ngrok auth token
NGROK_AUTH_TOKEN = "2tqZZ0zm7U5azTMJ6B7In96Eia0_7M44dRZx1KKTz8Dfd7R4Z"

# Set authentication token
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
print("âœ… ngrok Auth Token Set!")

public_url = ngrok.connect(8501).public_url
print(f"ğŸš€ Streamlit App is running at: {public_url}")

import requests

OLLAMA_API_URL = "http://127.0.0.1:11434/api/generate"

headers = {"Content-Type": "application/json"}
payload = {
    "model": "llama3.2",
    "prompt": "What is the capital of France?",
    "stream": False
}

response = requests.post(OLLAMA_API_URL, json=payload, headers=headers)

if response.status_code == 200:
    print("âœ… Response from Ollama:", response.json()["response"])
else:
    print(f"âŒ Error from Ollama: {response.status_code}, {response.text}")

!pip install streamlit requests tiktoken pandas matplotlib seaborn

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import requests
# import tempfile
# import os
# import tiktoken
# from datetime import datetime
# import pandas as pd
# import re
# import matplotlib.pyplot as plt
# from collections import Counter
# import random
# import seaborn as sns
# 
# OLLAMA_URL = "http://127.0.0.1:11434/api/generate"
# 
# # Streamlit UI setup
# st.set_page_config(page_title="Chat with Text File (Llama 3.2)", layout="wide")
# st.title("ğŸ“„ Chat with Your Text File (Llama 3.2)")
# st.caption("Upload a text file and get automatic chat statistics!")
# 
# # Upload the file
# uploaded_file = st.file_uploader("ğŸ“‚ Upload your text file", type=["txt"])
# 
# if uploaded_file:
#     st.success(f"âœ… Uploaded: {uploaded_file.name}")
# 
#     # Save the uploaded file temporarily
#     with tempfile.NamedTemporaryFile(delete=False, suffix=".txt") as temp_file:
#         temp_file.write(uploaded_file.read())
#         temp_txt_path = temp_file.name
# 
#     # Read the text file
#     with open(temp_txt_path, "r", encoding="utf-8") as f:
#         file_text = f.read()
# 
#     # Tokenize and display text information
#     enc = tiktoken.get_encoding("cl100k_base")
#     tokens = enc.encode(file_text)
#     st.write(f"Document length: {len(file_text)} characters.")
#     st.write(f"Number of tokens: {len(tokens)}")
# 
#     # Remove the temporary file
#     os.remove(temp_txt_path)
# 
#     # Display the text (first 1000 characters for preview)
#     with st.expander("ğŸ” View Extracted Text"):
#         st.text_area("Text Content", file_text[:1000] + "..." if len(file_text) > 1000 else file_text, height=200)
# 
#     def parse_chat(file_text):
#       messages = []
#       senders = set()  # To collect unique senders
#       lines = file_text.split('\n')
# 
#       for line in lines:
#         # Adjust regex to account for potential non-breaking spaces, emojis, and other special characters
#         match = re.match(r"(\d{1,2}/\d{1,2}/\d{2,4}),\s*(\d{1,2}:\d{2})\s*[â€¯]?(am|pm|AM|PM)\s*-\s*(.*?):\s*(.*)", line)
# 
# 
#         if match:
#           date = match.group(1)
#           time = match.group(2)
#           sender = match.group(4)
#           message = match.group(5)
#           timestamp = f"{date} {time}"
#           messages.append({"timestamp": timestamp, "sender": sender, "message": message})
#           senders.add(sender)
#         else:
#           # Debugging: Display the line that is being skipped
#           print(f"Skipping line: {line}")
# 
#       return messages, list(senders)
# 
# 
#     # Get the parsed messages and participant names
#     messages, senders = parse_chat(file_text)
# 
#     # Create a DataFrame from the messages
#     df = pd.DataFrame(messages)
#     df['timestamp'] = pd.to_datetime(df['timestamp'])
#     df['day_of_week'] = df['timestamp'].dt.day_name()
# 
#     # Refined system prompt for Llama
#     system_prompt = (
#         "You are an AI assistant that answers questions based on the given document."
#         "Give direct answers, provide numbers or percentages for statistics."
#         "Here are some questions you need to answer based on the chat history:\n"
#         "1. Give me the statistics data for the weekdays from the chat.\n"
#         "2. Based on the timestamp, check for the immediate responses in the chat.\n"
#         "3. Who uses more emojis in the chat?\n"
#         "4. Analyze the entire chat history from the first recorded message to the last recorded message, without limiting the timeframe."
#         "Identify the person who initiated the most conversations and provide a detailed breakdown, including the count of conversations started by each participant."
#         "What is the average response time during conversations?\n"
#         "5. Give me statistics of who focuses a lot on the chat?\n"
#         "6. What is the overall count of the total number of messages sent by each participant?\n"
#         "7. Top 3 most used words by each participant in the chat?\n"
#         "8. How many times did each participant start the conversation?\n"
#         "9. Who is likely to end the conversation first? Give me the count.\n"
#         "10. Statistics of who used the emojis more in total?\n"
#         "11. What is the total number of questions asked by each participant?\n"
#         "Answer all the above questions based on the chat data provided. Each answer should be clear and concise."
#         "You should also do sentiment analysis of the document uploaded, analyze emotions like 'sad','happy','anger','love','jealousy','care','disagreements','bored','funny','excitement' and other emotions."
#     )
# 
#     # Max tokens for Llama 3.2 is 128,000 tokens
#     max_tokens = 128000
#     max_chars = max_tokens * 4
# 
#     # Split text into chunks of 5000 characters (or a reasonable limit)
#     chunk_size = 5000
#     chunks = [file_text[i:i + chunk_size] for i in range(0, len(file_text), chunk_size)]
# 
#     responses = []
#     for chunk in chunks:
#         ollama_payload = {
#             "model": "llama3.2",
#             "prompt": f"{system_prompt}\n\nDocument:\n{chunk}\n\nAI Answer:",
#             "stream": False
#         }
# 
#         response = requests.post(OLLAMA_URL, json=ollama_payload)
# 
#         if response.status_code == 200:
#             result = response.json().get("response", "âš ï¸ No response from Llama 3.")
#             responses.append(result)
#         else:
#             st.error("âš ï¸ Failed to get response from Llama 3. Is Ollama running?")
#             break  # Stop further processing if thereâ€™s an error
# 
#     # Combine all responses
#     final_response = "\n\n".join(responses) if responses else "âš ï¸ No response from Llama 3."
# 
#     # Replace placeholders with actual names dynamically
#     for sender in senders:
#         final_response = final_response.replace("participant", sender)
# 
#     # Display the answers
#     st.subheader("ğŸ“Œ Automatic Chat Analysis Results:")
#     st.write(final_response)
# 
# 
# 
#     #emoji usage count calculatiion
#     emoji_usage = df['message'].apply(lambda x: sum([1 for c in x if c in "ğŸ™‚ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜‚ğŸ¤£ğŸ˜ŠğŸ˜ğŸ˜ğŸ¥ºğŸ™„ğŸ˜›ğŸ˜œğŸ¤”ğŸ˜³ğŸ‘€ğŸ™„ğŸ˜‘ğŸ˜ğŸ˜¬ğŸ‘ğŸ˜ŒğŸ™‚"]))  # Adjust emoji detection logic
#     emoji_count = emoji_usage.sum()
# 
#     #response time calculation
#     df['response_time'] = df['timestamp'].diff().shift(-1)  # Calculate time between messages
#     avg_response_time = df['response_time'].mean()
# 
#     #calculation of weekly activity activity chat
#     def get_time_slot(hour):
#       if 5 <= hour < 8:
#         return "Early morn."
#       elif 8 <= hour < 12:
#         return "Morning"
#       elif 12 <= hour < 17:
#         return "Afternoon"
#       elif 17 <= hour < 21:
#         return "Evening"
#       else:
#         return "Night"
# 
#     df['hour']=df['timestamp'].dt.hour
#     df['weekday']=df['timestamp'].dt.day_name().str[:3]
#     df['time_slot']=df['hour'].apply(get_time_slot)
# 
#     activity_matrix = df.groupby(['time_slot', 'weekday']).size().unstack(fill_value=0)
# 
#     # Step 4: Order rows and columns
#     slot_order = ["Early morn.", "Morning", "Afternoon", "Evening", "Late night"]
#     day_order = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
#     activity_matrix = activity_matrix.reindex(index=slot_order, columns=day_order)
# 
# 
# 
# 
#     # Dashboard Title
#     st.markdown("## ğŸ“Š Chat Dashboard")
# 
#     # Row 1: Key Stats
#     col1, col2, col3 = st.columns(3)
#     with col1:
#       st.metric("ğŸ“ˆTotal Messages", len(df))
#     with col2:
#       st.metric("ğŸ˜„Total Emojis", emoji_count)
#     with col3:
#       st.metric("â³Avg. Response Time", str(avg_response_time).split('.')[0])
# 
#     # Row 2: Bar + Focus Pie
#     col1, col2 = st.columns([2, 1])
#     with col1:
#       st.markdown("#### ğŸ“… Messages by Day")
#       day_stats = df['day_of_week'].value_counts()
#       st.write(day_stats)
#       fig, ax = plt.subplots(figsize=(3, 2))
#       day_stats.plot(kind='bar', color='skyblue', ax=ax)
#       ax.set_title('Messages Sent by Day of the Week', fontsize=10)
#       ax.set_xlabel('Day of the Week',fontsize=8)
#       ax.set_ylabel('Number of Messages',fontsize=8)
#       ax.tick_params(axis='x',labelrotation=45,labelsize=7)
#       ax.tick_params(axis='y',labelsize=7)
#       plt.tight_layout(pad=0.5)
#       st.pyplot(fig)
# 
#     with col2:
#       st.markdown("#### ğŸ¯ Focus Pie")
#       focus_counts=df['sender'].value_counts()
#       st.write(focus_counts)
#       def small_autopct(pct):
#         return f'{pct:.1f}%' if pct > 0 else ''
#       fig, ax = plt.subplots(figsize=(1.8, 1.8))
#       focus_counts.plot(kind='pie', ax=ax, autopct=small_autopct,colors=['#b38eda','#8ec5da'], textprops={'fontsize': 5})
#       ax.set_ylabel('')
#       ax.set_title('', fontsize=7)
#       ax.set_position([0.1, 0.1, 0.8, 0.8])
#       st.pyplot(fig)
# 
#     # Row 3: Sentiment Pie
#     col1,col2=st.columns([1,2])
#     with col1:
#       st.markdown("#### ğŸ‘» Sentiment Breakdown")
#       sentiments = ['happy', 'sad', 'angry', 'excited', 'bored','funny','love','care','disagreements']  # Placeholder, you can replace with actual sentiment analysis
#       sentiment_counts = Counter([random.choice(sentiments) for _ in range(len(df))])
#       st.write(sentiment_counts)
# 
#       fig, ax = plt.subplots(figsize=(2, 2))
#       sentiment_counts=Counter([random.choice(sentiments) for _ in range(len(df))])
#       sentiment_series=pd.Series(sentiment_counts)
#       def small_autopct(pct):
#         return f'{pct:.1f}%' if pct > 0 else ''
#       sentiment_series.plot(kind='pie', ax=ax, autopct=small_autopct,colors=['#ff9999','#66b3ff','#99ff99','#ffcc99','#21aed8','#c292ef','#ef92c1','#b97e7e','#d1e377'],textprops={'fontsize': 5})
#       ax.set_ylabel('')
#       ax.set_title('sentiments', fontsize=7)
#       ax.set_position([0.1, 0.1, 0.8, 0.8])
#       st.pyplot(fig)
# 
#     with col2:
#       st.markdown("#### â˜€ï¸ğŸŒ” weekly chat activity")
#       fig,ax=plt.subplots(figsize=(7,3.5))
#       sns.heatmap(activity_matrix, annot=True, fmt='.0f', cmap='YlOrBr', cbar=False, linewidths=0.5, linecolor='white', ax=ax)
#       ax.set_xlabel("day of the week",fontsize=10)
#       ax.set_ylabel("time of the day",fontsize=10)
#       ax.set_title("chat data by time and day",fontsize=10,pad=10)
# 
#       ax.tick_params(axis='x',labelrotation=0,labelsize=8)
#       ax.tick_params(axis='y',labelsize=8)
#       st.pyplot(fig)
# 
# 
# 
# 
#     # User's Question input
#     question = st.text_input("ğŸ’¡ Ask a question about the document")
# 
#     if question:
#         st.info("ğŸ¤– Sending question to Llama 3.2 :)...")
# 
#         # Refined system prompt to focus on direct answers only
#         system_prompt_user = (
#             "You are an AI assistant that answers questions based on the given document."
#             "When asked about a specific message, return only the exact date and time when the message was sent."
#             "If the message is not found or timestamped, respond with 'I don't know'."
#             "Do not provide any additional context, reasoning, or elaboration."
#             "Only return the direct date and time of the message. Do not include any other information."
#             "Perform sentiment analysis on the document uploaded, analyze the emotions such as 'upset','anger','jealous','care','love','emotional','happiness','sad','funny', and others."
#             "Provide accurate answers. Don't make things up."
#             "If the question is not related to the document, politely respond that you are tuned only to answer questions that are related to the document."
#         )
# 
#         # Max tokens for Llama 3.2 is 128,000 tokens
#         max_tokens = 128000
#         chunk_size = 5000  # Split text into chunks of 5000 characters (or a reasonable limit)
#         chunks = [file_text[i:i + chunk_size] for i in range(0, len(file_text), chunk_size)]
# 
#         responses_user_question = []
#         for chunk in chunks:
#             ollama_payload_user_question = {
#                 "model": "llama3.2",
#                 "prompt": f"{system_prompt_user}\n\nDocument:\n{chunk}\n\nUser Question: {question}\n\nAI Answer:",
#                 "stream": False
#             }
# 
#             response = requests.post(OLLAMA_URL, json=ollama_payload_user_question)
# 
#             if response.status_code == 200:
#                 result = response.json().get("response", "âš ï¸ No response from Llama 3.")
#                 responses_user_question.append(result)
#             else:
#                 st.error("âš ï¸ Failed to get response from Llama 3. Is Ollama running?")
#                 break
# 
#         # Combine all responses for the user's question
#         final_response_user_question = "\n\n".join(responses_user_question) if responses_user_question else "âš ï¸ No response from Llama 3."
# 
#         # Display the user's answer
#         st.subheader("ğŸ“Œ User's Question Answer:")
#         st.write(final_response_user_question)
#

!pip install tiktoken

!streamlit run app.py &

